import os
import shutil
import random
import numpy as np
import torch

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)

# =========================
# 0. ìºì‹œ ì„¤ì •
# =========================
HF_CACHE_DIR = r"C:\hf_cache"
os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR

# =========================
# 1. ê¸°ë³¸ ì„¤ì •
# =========================
MODEL_NAME = "KETI-AIR/ke-t5-small"
DATA_PATH = "slang_dataset_10000.csv"

OUTPUT_DIR = "./kcslang-stable-ckpt"
SAVE_DIR = "./kcslang-stable-model"

MAX_INPUT_LENGTH = 64
MAX_TARGET_LENGTH = 64
SEED = 42

# âœ… ì˜¤ë²„í• ë‹¨ê³„: 20ê°œë¡œ ë¨¼ì € ì„±ê³µì‹œí‚¨ ë’¤ 100â†’1000â†’ì „ì²´
TRAIN_SIZE = 5000
VALID_SIZE = 2000

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


# =========================
# 2. ë°ì´í„° ë¡œë“œ
# =========================
def load_small_split(csv_path: str, train_size: int, valid_size: int):
    raw = load_dataset(
        "csv",
        data_files={"data": csv_path},
        encoding="utf-8-sig",
    )["data"]

    raw = raw.shuffle(seed=SEED)

    total_need = train_size + valid_size
    if len(raw) < total_need:
        raise ValueError(f"ë°ì´í„° ê°œìˆ˜ê°€ ë¶€ì¡±í•¨: í˜„ì¬ {len(raw)}ê°œ, ìµœì†Œ {total_need}ê°œ í•„ìš”")

    train_ds = raw.select(range(train_size))
    valid_ds = raw.select(range(train_size, train_size + valid_size))

    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print("  - ì „ì²´:", len(raw))
    print("  - train:", len(train_ds))
    print("  - valid:", len(valid_ds))
    print("  - columns:", raw.column_names)
    return train_ds, valid_ds


# =========================
# 3. ì „ì²˜ë¦¬ (ì¤‘ìš”: </s> ê°™ì€ EOS í† í° ë„£ì§€ ì•Šê¸°)
# =========================
def preprocess_function(examples, tokenizer):
    inputs = [f"ì€ì–´: {s}\ní‘œì¤€ì–´:" for s in examples["source"]]
    targets = examples["target"]

    model_inputs = tokenizer(
        inputs,
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
        padding=False,  # âœ… collatorê°€ ë™ì íŒ¨ë”© ì²˜ë¦¬
    )

    labels = tokenizer(
        text_target=targets,
        max_length=MAX_TARGET_LENGTH,
        truncation=True,
        padding=False,
    )["input_ids"]

    model_inputs["labels"] = labels
    return model_inputs


def translate(model, tokenizer, text: str):
    model.eval()
    inp = tokenizer(
        f"ì€ì–´: {text}\ní‘œì¤€ì–´:",
        return_tensors="pt",
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
    ).to(DEVICE)

    with torch.no_grad():
        out = model.generate(
            **inp,
            max_new_tokens=64,
            num_beams=4,
            do_sample=False,
            no_repeat_ngram_size=3,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            early_stopping=True,
        )

    return tokenizer.decode(out[0], skip_special_tokens=True)


# =========================
# 4. ë©”ì¸
# =========================
def main():
    set_seed(SEED)

    # âœ… (í•„ìˆ˜) ì˜ˆì „ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ í•™ìŠµì´ ë§ê°€ì§„ ìƒíƒœë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒ
    # ì˜¤ë²„í• ì„±ê³µ í™•ì¸ ì „ê¹Œì§„ ë¬´ì¡°ê±´ ìƒˆë¡œ ì‹œì‘í•˜ì
    if os.path.isdir(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    if os.path.isdir(SAVE_DIR):
        shutil.rmtree(SAVE_DIR)

    train_dataset, valid_dataset = load_small_split(DATA_PATH, TRAIN_SIZE, VALID_SIZE)

    print("âœ… í† í¬ë‚˜ì´ì €/ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=HF_CACHE_DIR)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, cache_dir=HF_CACHE_DIR).to(DEVICE)

    # âœ… (í•„ìˆ˜) í•™ìŠµ ì „ sanity check: ë¼ë²¨ì´ ì •ìƒì¸ì§€ í™•ì¸
    ex = train_dataset[0]
    print("\n[SANITY CHECK]")
    print("SRC:", ex["source"])
    print("TGT:", ex["target"])
    lab_ids = tokenizer(text_target=ex["target"], truncation=True, max_length=MAX_TARGET_LENGTH)["input_ids"]
    print("label_len:", len(lab_ids))
    print("label_decoded:", tokenizer.decode(lab_ids, skip_special_tokens=True))

    print("\nâœ… ì „ì²˜ë¦¬ ì¤‘...")
    tokenized_train = train_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=train_dataset.column_names,
    )
    tokenized_valid = valid_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=valid_dataset.column_names,
    )

    # âœ… ë™ì íŒ¨ë”© + label padëŠ” -100
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        label_pad_token_id=-100,
    )

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,

        # âœ… ì˜¤ë²„í•ì€ ì˜¤ë˜ ëŒë ¤ì„œ ë¶™ëŠ”ì§€ í™•ì¸
        num_train_epochs=50,

        # âœ… ì˜¤ë²„í•ì—ì„œëŠ” accë¥¼ í¬ê²Œ ì¡ì§€ ë§ì (ë¶ˆì•ˆì •í•´ì§)
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,  # âœ… 16 â†’ 4ë¡œ ê°ì†Œ

        # âœ… T5 ì•ˆì •í™”
        adafactor=True,
        learning_rate=1e-6,
        warmup_ratio=0.1,
        max_grad_norm=0.2,

        weight_decay=0.0,
        logging_steps=2,
        save_steps=10**9,  # âœ… ì˜¤ë²„í• ë‹¨ê³„ì—ì„œëŠ” ì €ì¥ ê±°ì˜ ì•ˆí•¨
        fp16=False,
        seed=SEED,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_valid,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    print("ğŸš€ í•™ìŠµ ì‹œì‘! (resume_from_checkpoint=False)")
    trainer.train(resume_from_checkpoint=False)

    os.makedirs(SAVE_DIR, exist_ok=True)
    trainer.save_model(SAVE_DIR)
    tokenizer.save_pretrained(SAVE_DIR)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_DIR}")

    print("\n==============================")
    print("âœ… QUICK GENERATION TEST (TRAIN SAMPLES)")
    print("==============================")
    samples = train_dataset.select(range(min(5, len(train_dataset))))
    for ex in samples:
        src = ex["source"]
        tgt = ex["target"]
        pred = translate(model, tokenizer, src)
        print("\nSRC:", src)
        print("TGT:", tgt)
        print("PRD:", pred)


if __name__ == "__main__":
    main()
