import os, random
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    TrainingArguments, Trainer, DataCollatorForSeq2Seq
)

HF_CACHE_DIR = r"C:\hf_cache"
os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR

MODEL_NAME = "KETI-AIR/ke-t5-small"
DATA_PATH = "slang_dataset_10000.csv"

MAX_INPUT_LENGTH = 128
MAX_TARGET_LENGTH = 128
SEED = 42

def set_seed(seed=42):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def preprocess_function(examples, tokenizer):
    inputs = ["í‘œì¤€ì–´: " + s for s in examples["source"]]
    targets = examples["target"]

    model_inputs = tokenizer(
        inputs,
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
        padding="max_length",
    )

    # âœ… ê¶Œì¥ ë°©ì‹
    labels = tokenizer(
        text_target=targets,
        max_length=MAX_TARGET_LENGTH,
        truncation=True,
        padding="max_length",
    )["input_ids"]

    # âœ… padëŠ” lossì—ì„œ ë¬´ì‹œí•˜ë„ë¡ -100ìœ¼ë¡œ (ì•ˆì •ì ìœ¼ë¡œ)
    labels = [
        [(t if t != tokenizer.pad_token_id else -100) for t in lab]
        for lab in labels
    ]

    model_inputs["labels"] = labels
    return model_inputs

def generate_one(model, tokenizer, device, src: str):
    inp = tokenizer("í‘œì¤€ì–´: " + src, return_tensors="pt", truncation=True, max_length=128).to(device)
    with torch.no_grad():
        out = model.generate(
            **inp,
            max_new_tokens=64,
            num_beams=4,
            do_sample=False,
            no_repeat_ngram_size=3,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            early_stopping=True,
        )
    return tokenizer.decode(out[0], skip_special_tokens=True)

def main():
    set_seed(SEED)

    raw = load_dataset("csv", data_files={"data": DATA_PATH}, encoding="utf-8-sig")["data"]
    raw = raw.shuffle(seed=SEED)

    # âœ… ì¼ë¶€ë§Œ ë–¼ì„œ â€œí•™ìŠµì´ ë˜ëŠ”ì§€â€ í™•ì¸
    train_ds = raw.select(range(100))
    valid_ds = raw.select(range(100, 120))

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=HF_CACHE_DIR)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, cache_dir=HF_CACHE_DIR)

    tokenized_train = train_ds.map(lambda x: preprocess_function(x, tokenizer), batched=True,
                                   remove_columns=train_ds.column_names)
    tokenized_valid = valid_ds.map(lambda x: preprocess_function(x, tokenizer), batched=True,
                                   remove_columns=valid_ds.column_names)

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding="longest")

    args = TrainingArguments(
        output_dir="./overfit-ckpt",
        overwrite_output_dir=True,
        num_train_epochs=30,            # âœ… ì˜¤ë²„í”¼íŒ… ëª©ì 
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        learning_rate=3e-5,
        warmup_ratio=0.1,
        max_grad_norm=0.5,
        weight_decay=0.0,
        logging_steps=5,
        save_steps=10**9,               # âœ… ì‚¬ì‹¤ìƒ ì €ì¥ ì•ˆ í•˜ê²Œ
        fp16=False,                     # âœ… ì¼ë‹¨ fp16 ë„ê³  ì•ˆì •ì„± í™•ì¸
        seed=SEED,
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_valid,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    print("ğŸš€ ì˜¤ë²„í”¼íŒ… í•™ìŠµ ì‹œì‘!")
    trainer.train()

    # âœ… í•™ìŠµ ë°ì´í„° 5ê°œë¡œ ìƒì„± í™•ì¸
    model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    print("\n==============================")
    print("âœ… TRAIN OVERFIT GENERATION CHECK")
    print("==============================")

    samples = train_ds.select(range(5))
    for ex in samples:
        src = ex["source"]
        tgt = ex["target"]
        pred = generate_one(model, tokenizer, device, src)
        print("\nSRC:", src)
        print("TGT:", tgt)
        print("PRD:", pred)

if __name__ == "__main__":
    main()
